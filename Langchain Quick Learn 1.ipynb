{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11a243e-2b9c-40b7-b163-f209cd5f83ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e34d7-6b97-46cb-95df-91348ba97a89",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba027a77-4c0e-4b9e-8554-f17dd095e645",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98272888-383f-48e3-9906-070a8f1282e9",
   "metadata": {},
   "source": [
    "### Chat Models: GPT 3.5 Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c6ea1d-9ce3-4301-983f-325db2432771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cafce92c-144e-44d4-8343-3b98f62fb60d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles, characterized by the principles of superposition, quantization, and uncertainty.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI()\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-4-turbo-preview')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87b54cf1-04ee-4e63-926c-b1ef4b55a8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChatOpenAI in module langchain_openai.chat_models.base:\n",
      "\n",
      "class ChatOpenAI(langchain_core.language_models.chat_models.BaseChatModel)\n",
      " |  ChatOpenAI(*, name: Optional[str] = None, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, async_client: Any = None, model: str = 'gpt-3.5-turbo', temperature: float = 0.7, model_kwargs: Dict[str, Any] = None, api_key: Optional[str] = None, base_url: Optional[str] = None, organization: Optional[str] = None, openai_proxy: Optional[str] = None, timeout: Union[float, Tuple[float, float], Any, NoneType] = None, max_retries: int = 2, streaming: bool = False, n: int = 1, max_tokens: Optional[int] = None, tiktoken_model_name: Optional[str] = None, default_headers: Optional[Mapping[str, str]] = None, default_query: Optional[Mapping[str, object]] = None, http_client: Optional[Any] = None) -> None\n",
      " |  \n",
      " |  `OpenAI` Chat large language models API.\n",
      " |  \n",
      " |  To use, you should have the\n",
      " |  environment variable ``OPENAI_API_KEY`` set with your API key.\n",
      " |  \n",
      " |  Any parameters that are valid to be passed to the openai.create call can be passed\n",
      " |  in, even if not explicitly saved on this class.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain_community.chat_models import ChatOpenAI\n",
      " |          openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ChatOpenAI\n",
      " |      langchain_core.language_models.chat_models.BaseChatModel\n",
      " |      langchain_core.language_models.base.BaseLanguageModel\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  bind_functions(self, functions: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', function_call: \"Optional[Union[_FunctionCall, str, Literal['auto', 'none']]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
      " |      Bind functions (and other objects) to this chat model.\n",
      " |      \n",
      " |      Assumes model is compatible with OpenAI function-calling API.\n",
      " |      \n",
      " |      NOTE: Using bind_tools is recommended instead, as the `functions` and\n",
      " |          `function_call` request parameters are officially marked as deprecated by\n",
      " |          OpenAI.\n",
      " |      \n",
      " |      Args:\n",
      " |          functions: A list of function definitions to bind to this chat model.\n",
      " |              Can be  a dictionary, pydantic model, or callable. Pydantic\n",
      " |              models and callables will be automatically converted to\n",
      " |              their schema dictionary representation.\n",
      " |          function_call: Which function to require the model to call.\n",
      " |              Must be the name of the single provided function or\n",
      " |              \"auto\" to automatically determine which function to call\n",
      " |              (if any).\n",
      " |          kwargs: Any additional parameters to pass to the\n",
      " |              :class:`~langchain.runnable.Runnable` constructor.\n",
      " |  \n",
      " |  bind_tools(self, tools: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', *, tool_choice: \"Optional[Union[dict, str, Literal['auto', 'none']]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
      " |      Bind tool-like objects to this chat model.\n",
      " |      \n",
      " |      Assumes model is compatible with OpenAI tool-calling API.\n",
      " |      \n",
      " |      Args:\n",
      " |          tools: A list of tool definitions to bind to this chat model.\n",
      " |              Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
      " |              models, callables, and BaseTools will be automatically converted to\n",
      " |              their schema dictionary representation.\n",
      " |          tool_choice: Which tool to require the model to call.\n",
      " |              Must be the name of the single provided function or\n",
      " |              \"auto\" to automatically determine which function to call\n",
      " |              (if any), or a dict of the form:\n",
      " |              {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
      " |          kwargs: Any additional parameters to pass to the\n",
      " |              :class:`~langchain.runnable.Runnable` constructor.\n",
      " |  \n",
      " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      " |      Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n",
      " |      \n",
      " |      Official documentation: https://github.com/openai/openai-cookbook/blob/\n",
      " |      main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
      " |  \n",
      " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      " |      Get the tokens present in the text with tiktoken package.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.main.ModelMetaclass\n",
      " |      Build extra kwargs from additional params that were passed in.\n",
      " |  \n",
      " |  get_lc_namespace() -> 'List[str]' from pydantic.main.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |  \n",
      " |  is_lc_serializable() -> 'bool' from pydantic.main.ModelMetaclass\n",
      " |      Return whether this model can be serialized by Langchain.\n",
      " |  \n",
      " |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      " |      Validate that api key and python package exists in environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |      \n",
      " |      These attributes must be accepted by the constructor.\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |      \n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'langchain_openai.chat_models.base.ChatOpenAI.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'async_client': 'Any', 'client': 'Any', 'default_he...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'langchain_openai.chat_models.base.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'async_client': True, 'callback_manager': True, ...\n",
      " |  \n",
      " |  __fields__ = {'async_client': ModelField(name='async_client', type=Opt...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  __post_root_validators__ = [(False, <function BaseChatModel.raise_depr...\n",
      " |  \n",
      " |  __pre_root_validators__ = [<function ChatOpenAI.build_extra>]\n",
      " |  \n",
      " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, name: Optional[str] = None, cache...Non...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  __call__(self, messages: 'List[BaseMessage]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  async agenerate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |      \n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the runnable did not implement a native async version of invoke.\n",
      " |      \n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |  \n",
      " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[BaseMessageChunk]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  call_as_llm(self, message: 'str', stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      " |      Return a dictionary of the LLM.\n",
      " |  \n",
      " |  generate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: A config to use when invoking the runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The output of the runnable.\n",
      " |  \n",
      " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[BaseMessageChunk]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  OutputType\n",
      " |      Get the output type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  __orig_bases__ = (langchain_core.language_models.base.BaseLanguageMode...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |      \n",
      " |      Useful for checking if an input will fit in a model's context window.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The integer number of tokens in the text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  InputType\n",
      " |      Get the input type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |  \n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __init__(self, **kwargs: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __repr_args__(self) -> Any\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |      \n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |  \n",
      " |  to_json(self) -> Union[langchain_core.load.serializable.SerializedConstructor, langchain_core.load.serializable.SerializedNotImplemented]\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_id() -> List[str] from pydantic.main.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |      \n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> 'unicode'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |  \n",
      " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      [*Beta*]  Generate a stream of events.\n",
      " |      \n",
      " |      Use to create an iterator ove StreamEvents that provide real-time information\n",
      " |      about the progress of the runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |      \n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |      \n",
      " |      * ``event``: str - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      * ``name``: str - The name of the runnable that generated the event.\n",
      " |      * ``run_id``: str - randomly generated ID associated with the given execution of\n",
      " |          the runnable that emitted the event.\n",
      " |          A child runnable that gets invoked as part of the execution of a\n",
      " |          parent runnable is assigned its own unique ID.\n",
      " |      * ``tags``: Optional[List[str]] - The tags of the runnable that generated\n",
      " |          the event.\n",
      " |      * ``metadata``: Optional[Dict[str, Any]] - The metadata of the runnable\n",
      " |          that generated the event.\n",
      " |      * ``data``: Dict[str, Any]\n",
      " |      \n",
      " |      \n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |      \n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      |----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      \n",
      " |      Here are declarations associated with the events shown above:\n",
      " |      \n",
      " |      `format_docs`:\n",
      " |      \n",
      " |      ```python\n",
      " |      def format_docs(docs: List[Document]) -> str:\n",
      " |          '''Format the docs.'''\n",
      " |          return \", \".join([doc.page_content for doc in docs])\n",
      " |      \n",
      " |      format_docs = RunnableLambda(format_docs)\n",
      " |      ```\n",
      " |      \n",
      " |      `some_tool`:\n",
      " |      \n",
      " |      ```python\n",
      " |      @tool\n",
      " |      def some_tool(x: int, y: str) -> dict:\n",
      " |          '''Some_tool.'''\n",
      " |          return {\"x\": x, \"y\": y}\n",
      " |      ```\n",
      " |      \n",
      " |      `prompt`:\n",
      " |      \n",
      " |      ```python\n",
      " |      template = ChatPromptTemplate.from_messages(\n",
      " |          [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |      ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      ```\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |      \n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |      \n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
      " |          ]\n",
      " |      \n",
      " |          # will produce the following events (run_id has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          version: The version of the schema to use.\n",
      " |                   Currently only version 1 is available.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An async stream of StreamEvents.[*Beta*] Generate a stream of events.\n",
      " |      \n",
      " |      Use to create an iterator ove StreamEvents that provide real-time information\n",
      " |      about the progress of the runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |      \n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |      \n",
      " |      * ``event``: str - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      * ``name``: str - The name of the runnable that generated the event.\n",
      " |      * ``run_id``: str - randomly generated ID associated with the given execution of\n",
      " |          the runnable that emitted the event.\n",
      " |          A child runnable that gets invoked as part of the execution of a\n",
      " |          parent runnable is assigned its own unique ID.\n",
      " |      * ``tags``: Optional[List[str]] - The tags of the runnable that generated\n",
      " |          the event.\n",
      " |      * ``metadata``: Optional[Dict[str, Any]] - The metadata of the runnable\n",
      " |          that generated the event.\n",
      " |      * ``data``: Dict[str, Any]\n",
      " |      \n",
      " |      \n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |      \n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      |----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      \n",
      " |      Here are declarations associated with the events shown above:\n",
      " |      \n",
      " |      `format_docs`:\n",
      " |      \n",
      " |      ```python\n",
      " |      def format_docs(docs: List[Document]) -> str:\n",
      " |          '''Format the docs.'''\n",
      " |          return \", \".join([doc.page_content for doc in docs])\n",
      " |      \n",
      " |      format_docs = RunnableLambda(format_docs)\n",
      " |      ```\n",
      " |      \n",
      " |      `some_tool`:\n",
      " |      \n",
      " |      ```python\n",
      " |      @tool\n",
      " |      def some_tool(x: int, y: str) -> dict:\n",
      " |          '''Some_tool.'''\n",
      " |          return {\"x\": x, \"y\": y}\n",
      " |      ```\n",
      " |      \n",
      " |      `prompt`:\n",
      " |      \n",
      " |      ```python\n",
      " |      template = ChatPromptTemplate.from_messages(\n",
      " |          [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |      ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      ```\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |      \n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |      \n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
      " |          ]\n",
      " |      \n",
      " |          # will produce the following events (run_id has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          version: The version of the schema to use.\n",
      " |                   Currently only version 1 is available.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An async stream of StreamEvents.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |  \n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |      \n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |      \n",
      " |      The jsonpatch ops can be applied in order to construct state.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          diff: Whether to yield diffs between each step, or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |  \n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this runnable accepts specified as a pydantic model.\n",
      " |      \n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |  \n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this runnable.\n",
      " |  \n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate input to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic input schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |  \n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the runnable.\n",
      " |  \n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |  \n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
      " |  \n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |  \n",
      " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |  \n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |  \n",
      " |  with_listeners(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Called before the runnable starts running, with the Run object.\n",
      " |      on_end: Called after the runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the runnable throws an error, with the Run object.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |  \n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original runnable on exceptions.\n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original runnable on exceptions.\n",
      " |  \n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  config_specs\n",
      " |      List configurable fields for this runnable.\n",
      " |  \n",
      " |  input_schema\n",
      " |      The type of input this runnable accepts specified as a pydantic model.\n",
      " |  \n",
      " |  output_schema\n",
      " |      The type of output this runnable produces specified as a pydantic model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  name = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60748ce4-f461-421f-afc7-92a7dfa3b7d5",
   "metadata": {},
   "source": [
    "### Chat Completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a368cda-a09e-4753-a16e-9f5f48528558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ebef1de-057f-4121-bc0a-61184834b1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ,  ,       \n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in Hindi'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.'),\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf2f15-ad46-4b98-aa77-73656b1b17cc",
   "metadata": {},
   "source": [
    "### Caching LLM Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116adfa2-8689-4f08-a423-8aca98344d47",
   "metadata": {},
   "source": [
    "#### In-Memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69986873-3704-4469-9cd3-74334f7a106e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb5e01c-17cf-405b-b7a4-0421efb7a374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 3.97 ms, total: 18.3 ms\n",
      "Wall time: 1.17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy did the banana go to the doctor? Because it wasn't peeling well!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = 'Tell me a joke that toddler can understand'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dcbb179-d46d-438d-acec-8b50fe982af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 559 s, sys: 144 s, total: 703 s\n",
      "Wall time: 714 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy did the banana go to the doctor? Because it wasn't peeling well!\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9faa3de-2f6d-45c9-b837-cc501cc64473",
   "metadata": {},
   "source": [
    "#### SQLite Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbfbada1-20f1-49bd-bcf2-98954fa5c985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path='.lanchain.db'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc13ed4-8cad-4650-906d-c014af5fe85d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27 ms, sys: 5.43 ms, total: 32.4 ms\n",
      "Wall time: 920 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke('Tell me joke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfbbd10a-8873-4ec5-a873-7b52c7716a67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.04 ms, sys: 963 s, total: 3 ms\n",
      "Wall time: 2.09 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke('Tell me joke')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db756f4f-2960-4005-a781-c06b3447d7d3",
   "metadata": {},
   "source": [
    "### LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96fbc321-d74c-41d6-8b74-920e49a8b9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a rock song about the Moon and a Raven.'\n",
    "# print(llm.invoke(prompt).content) #direct output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dba245be-6206-4ee1-993b-1f5a1556aef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "In the dead of night, when the sky is black\n",
      "The moon shines bright, casting shadows back\n",
      "A raven calls out, a haunting sound\n",
      "As it flies high, above the ground\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dance in the night\n",
      "A mystical sight, a beautiful sight\n",
      "Their spirits intertwined, forever bound\n",
      "In the darkness, they are found\n",
      "\n",
      "Verse 2:\n",
      "The moon's glow, a silver light\n",
      "Guiding the raven, through the night\n",
      "The raven's cry, a mournful tune\n",
      "Echoing under the pale moon\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dance in the night\n",
      "A mystical sight, a beautiful sight\n",
      "Their spirits intertwined, forever bound\n",
      "In the darkness, they are found\n",
      "\n",
      "Bridge:\n",
      "They soar together, in the night sky\n",
      "A bond unbroken, as the years go by\n",
      "The moon and raven, a timeless pair\n",
      "In the darkness, they share\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dance in the night\n",
      "A mystical sight, a beautiful sight\n",
      "Their spirits intertwined, forever bound\n",
      "In the darkness, they are found\n",
      "\n",
      "Outro:\n",
      "Moon and raven, a tale so old\n",
      "Their story forever told\n",
      "In the night, they will always be\n",
      "Moon and raven, wild and free."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa970b4-416c-4613-b84b-4f6a67383707",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "559063b5-e991-4b47-a8e3-db629f985e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an experienced virologist. \\nWrite a few sentences about the following virus \"hiv\" in odia'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = '''You are an experienced virologist. \n",
    "Write a few sentences about the following virus \"{virus}\" in {language}'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(virus='hiv', language='odia')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a16e22d0-4ba3-486d-889d-f039bdc60bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIV                           \n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac23b1d-2077-403d-87da-1204aaa4275b",
   "metadata": {},
   "source": [
    "### ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dab6e95d-0027-451d-b542-c07f5fb4bd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf740325-b4ec-4cc7-aca8-58e855a34f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You response only in json format'), HumanMessage(content='Top 10 countries in Asia by population.')]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You response only in json format'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n='10', area='Asia')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d69e025a-0af9-42ef-9899-249404f2885e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": {\n",
      "        \"country\": \"China\",\n",
      "        \"population\": 1439323776\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"country\": \"India\",\n",
      "        \"population\": 1380004385\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"country\": \"United States\",\n",
      "        \"population\": 331002651\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"country\": \"Indonesia\",\n",
      "        \"population\": 273523615\n",
      "    },\n",
      "    \"5\": {\n",
      "        \"country\": \"Pakistan\",\n",
      "        \"population\": 220892340\n",
      "    },\n",
      "    \"6\": {\n",
      "        \"country\": \"Brazil\",\n",
      "        \"population\": 212559417\n",
      "    },\n",
      "    \"7\": {\n",
      "        \"country\": \"Nigeria\",\n",
      "        \"population\": 206139589\n",
      "    },\n",
      "    \"8\": {\n",
      "        \"country\": \"Bangladesh\",\n",
      "        \"population\": 164689383\n",
      "    },\n",
      "    \"9\": {\n",
      "        \"country\": \"Russia\",\n",
      "        \"population\": 145934462\n",
      "    },\n",
      "    \"10\": {\n",
      "        \"country\": \"Mexico\",\n",
      "        \"population\": 128932753\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b431e83-e97c-425f-8d95-a3b1cad57f77",
   "metadata": {},
   "source": [
    "### Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f603782-af16-4519-b54f-3d6ba574b307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3738e64b-86eb-420b-a4a6-ef463609dc46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an experienced virologist. \\nWrite a few sentences about the following virus \"hiv\" in odia'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI()\n",
    "template = '''You are an experienced virologist. \n",
    "Write a few sentences about the following virus \"{virus}\" in {language}'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(virus='hiv', language='odia')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a4fa09a-349d-4ea4-a2eb-c03e126f8d70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an experienced virologist. \n",
      "Write a few sentences about the following virus \"HSV\" in Gujrati\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'virus': 'HSV', 'language': 'Gujrati', 'text': 'HSV                  .       1   2     .                      .'}\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'virus':'HSV', 'language':'Gujrati'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7600b-d153-4b78-b23d-a6d1d9af6ef9",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1a141f8-74d7-4334-9bf6-ab30a6ea8548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2cdc54a8-17e6-481d-a084-b5016888e642",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure! Here is a simple implementation of linear regression in Python:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(X, y):\n",
      "    n = X.shape[0]\n",
      "    ones = np.ones((n, 1))\n",
      "    X = np.hstack((ones, X))\n",
      "    \n",
      "    beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "    \n",
      "    return beta\n",
      "\n",
      "# Example usage\n",
      "X = np.array([[1], [2], [3], [4], [5]])\n",
      "y = np.array([2, 4, 6, 8, 10])\n",
      "\n",
      "beta = linear_regression(X, y)\n",
      "print(\"Intercept:\", beta[0])\n",
      "print(\"Slope:\", beta[1])\n",
      "```\n",
      "\n",
      "This function takes in a matrix `X` containing the input features and a vector `y` containing the target values. It then calculates the coefficients of the linear regression model using the normal equation method and returns them. The example usage demonstrates how to use the function with sample data.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe provided Python function named `linear_regression` implements the linear regression model calculation utilizing the normal equation method. Linear regression is a fundamental statistical and machine learning approach used to model the relationship between a dependent variable and one or more independent variables. The goal is to find the linear equation that best fits the distribution of input-output pairs. The overarching aim is predicting the value of the dependent variable for given values of the independent variables.\n",
      "\n",
      "Here's a step-by-step explanation of how the function works:\n",
      "\n",
      "### Dependencies\n",
      "- The function depends on the `numpy` library, which is a popular Python library for numerical computing. It provides efficient data structures for multi-dimensional arrays (`ndarray`) and functions for performing operations on these structures.\n",
      "\n",
      "### Function Signature\n",
      "- `linear_regression(X, y)`: The function takes in two parameters:\n",
      "    - `X`: A 2D numpy array where each row represents a sample and each column represents a feature of the sample.\n",
      "    - `y`: A 1D numpy array (or vector) that represents the target values or outputs associated with each sample in `X`.\n",
      "\n",
      "### Process Steps:\n",
      "1. **Initialization**:\n",
      "    - `n = X.shape[0]`: Determines the number of samples in the dataset by extracting the size of the first dimension of `X`.\n",
      "    - `ones = np.ones((n, 1))`: Creates a column vector (numpy array) of ones with the same height as `X`. This column vector is used for incorporating the bias term (intercept) into the model.\n",
      "\n",
      "2. **Feature Augmentation**:\n",
      "    - `X = np.hstack((ones, X))`: Augments the original feature matrix `X` with a column of ones. This operation essentially appends a column on the left-hand side of `X`. This augmentation allows the model to include an intercept term in the linear equation.\n",
      "\n",
      "3. **Model Coefficient Calculation (Theta or Beta values)**:\n",
      "    - `beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)`: Computes the coefficients (parameters) of the linear regression model using the normal equation method.\n",
      "        - `X.T.dot(X)`: Calculates the dot product of the transpose of `X` with `X`, a key component in the normal equation.\n",
      "        - `np.linalg.inv(...)`: Finds the inverse of the matrix resulting from `X.T.dot(X)`.\n",
      "        - `.dot(X.T).dot(y)`: Multiplies the inverse of the above matrix with the transpose of `X` and then with the target vector `y`.\n",
      "    - The result, `beta`, is a numpy array where the first element represents the intercept term of the linear model, and the remaining elements correspond to the coefficients (slopes) of the respective features.\n",
      "\n",
      "### Example Usage:\n",
      "The provided example demonstrates how to utilize this function with a simple dataset consisting of inputs (`X`) and outputs (`y`). \n",
      "\n",
      "- The input features (`X`) range from 1 to 5 in a single-dimensional feature space. \n",
      "- The output (`y`) is directly proportional to the input, following a simple linear relationship (`y = 2x`).\n",
      "\n",
      "After calling `linear_regression(X, y)`, the function calculates and prints the `Intercept` and `Slope` of the linearly fitted model. Given the input data, it's expected to find an intercept of 0 (or very close to it, allowing for numerical precision issues) and a slope of 2, which perfectly models the relationship (`y = 2x`) indicated by the sample dataset.\n",
      "\n",
      "### Conclusion\n",
      "This implementation is an illustrative and straightforward example of linear regression applied to a simplistic problem. The normal equation approach, while not always efficient for very large datasets due to computational cost (inversing a large matrix can be very computational intensive), demonstrates the mathematical foundations of linear regression in a clear and instructive manner.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm1 = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.5)\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist and python programmer. Write a function that implements the concept of {concept}'\n",
    ")\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "llm2 = ChatOpenAI(model='gpt-4-turbo-preview', temperature=1.2)\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the python function {function}, describe it as detailed as possible'\n",
    ")\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "output = overall_chain.invoke('linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "734dfdef-6f69-4a47-87d0-ed9b721342bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided Python function named `linear_regression` implements the linear regression model calculation utilizing the normal equation method. Linear regression is a fundamental statistical and machine learning approach used to model the relationship between a dependent variable and one or more independent variables. The goal is to find the linear equation that best fits the distribution of input-output pairs. The overarching aim is predicting the value of the dependent variable for given values of the independent variables.\n",
      "\n",
      "Here's a step-by-step explanation of how the function works:\n",
      "\n",
      "### Dependencies\n",
      "- The function depends on the `numpy` library, which is a popular Python library for numerical computing. It provides efficient data structures for multi-dimensional arrays (`ndarray`) and functions for performing operations on these structures.\n",
      "\n",
      "### Function Signature\n",
      "- `linear_regression(X, y)`: The function takes in two parameters:\n",
      "    - `X`: A 2D numpy array where each row represents a sample and each column represents a feature of the sample.\n",
      "    - `y`: A 1D numpy array (or vector) that represents the target values or outputs associated with each sample in `X`.\n",
      "\n",
      "### Process Steps:\n",
      "1. **Initialization**:\n",
      "    - `n = X.shape[0]`: Determines the number of samples in the dataset by extracting the size of the first dimension of `X`.\n",
      "    - `ones = np.ones((n, 1))`: Creates a column vector (numpy array) of ones with the same height as `X`. This column vector is used for incorporating the bias term (intercept) into the model.\n",
      "\n",
      "2. **Feature Augmentation**:\n",
      "    - `X = np.hstack((ones, X))`: Augments the original feature matrix `X` with a column of ones. This operation essentially appends a column on the left-hand side of `X`. This augmentation allows the model to include an intercept term in the linear equation.\n",
      "\n",
      "3. **Model Coefficient Calculation (Theta or Beta values)**:\n",
      "    - `beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)`: Computes the coefficients (parameters) of the linear regression model using the normal equation method.\n",
      "        - `X.T.dot(X)`: Calculates the dot product of the transpose of `X` with `X`, a key component in the normal equation.\n",
      "        - `np.linalg.inv(...)`: Finds the inverse of the matrix resulting from `X.T.dot(X)`.\n",
      "        - `.dot(X.T).dot(y)`: Multiplies the inverse of the above matrix with the transpose of `X` and then with the target vector `y`.\n",
      "    - The result, `beta`, is a numpy array where the first element represents the intercept term of the linear model, and the remaining elements correspond to the coefficients (slopes) of the respective features.\n",
      "\n",
      "### Example Usage:\n",
      "The provided example demonstrates how to utilize this function with a simple dataset consisting of inputs (`X`) and outputs (`y`). \n",
      "\n",
      "- The input features (`X`) range from 1 to 5 in a single-dimensional feature space. \n",
      "- The output (`y`) is directly proportional to the input, following a simple linear relationship (`y = 2x`).\n",
      "\n",
      "After calling `linear_regression(X, y)`, the function calculates and prints the `Intercept` and `Slope` of the linearly fitted model. Given the input data, it's expected to find an intercept of 0 (or very close to it, allowing for numerical precision issues) and a slope of 2, which perfectly models the relationship (`y = 2x`) indicated by the sample dataset.\n",
      "\n",
      "### Conclusion\n",
      "This implementation is an illustrative and straightforward example of linear regression applied to a simplistic problem. The normal equation approach, while not always efficient for very large datasets due to computational cost (inversing a large matrix can be very computational intensive), demonstrates the mathematical foundations of linear regression in a clear and instructive manner.\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e4c3d-273f-4289-b798-e6838419127b",
   "metadata": {},
   "source": [
    "### Langchain Agents in Action: Python REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a567399-0c14-43d1-a3c4-e62d5ba683dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dc65c5b-fc1c-47a1-9742-54bc9c9e234b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[13, 26, 39, 52, 65, 78, 91]\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n for n in range(1,100) if n%13 == 0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0bfff33-c379-46fc-8835-4387b2c310d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo solve this, I will first calculate the factorial of 12 using the `math` module's `factorial` function. Then, I will calculate the square root of that result using the `sqrt` function from the same module. Finally, I will format the result to display it with 4 decimal points using the `format` function.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that the `math` module is imported, I can calculate the factorial of 12.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: factorial_12 = math.factorial(12)\n",
      "print(factorial_12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m479001600\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have the factorial of 12, I can calculate its square root.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: sqrt_factorial_12 = math.sqrt(factorial_12)\n",
      "print(sqrt_factorial_12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21886.105181141756\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have the square root of the factorial of 12, I can format it to display with 4 decimal points.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: print(f\"{sqrt_factorial_12:.4f}\")\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21886.1052\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "\n",
      "Final Answer: 21886.1052\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the sqaure root of the factorial of 12 and display it with 4 decimal points',\n",
       " 'output': '21886.1052'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor.invoke('Calculate the sqaure root of the factorial of 12 and display it with 4 decimal points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd17bc5e-7696-4dfb-8719-5dfb50e85bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to calculate 5.1 raised to the power of 7.3 to get the answer.\n",
      "Action: Python_REPL\n",
      "Action Input: print(5.1**7.3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke('What is the answer to 5.1**7.3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e1c0441-c533-4040-a0f5-74ea4061bd8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the answer to 5.1**7.3?', 'output': '146306.05007233328'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2960578-0b73-4267-a37b-9536ce1eeba6",
   "metadata": {},
   "source": [
    "### LangChain Tools: DuckDuckGo and Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cd1fd9b-9496-42e3-994e-809dbff9d7ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Google, duckduckgo, tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37c69fc7-6e77-454a-8790-22a910031cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.4.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.4.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f251a2b4-f3dd-4d22-acd3-664a2766c64b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subhas Chandra Bose (/   b  h  s  t  n d r   b o s /  shuub-HAHSS CHUN-dr BOHSS; 23 January 1897 - 18 August 1945) was an Indian nationalist whose defiance of British authority in India made him a hero among many Indians, but his wartime alliances with Nazi Germany and Imperial Japan left a legacy vexed by authoritarianism, anti-Semitism, and military failure. Subhas Chandra Bose (born c. January 23, 1897, Cuttack, Orissa [now Odisha], Indiadied August 18, 1945, Taipei, Taiwan?) was an Indian revolutionary prominent in the independence movement against British rule of India.He also led an Indian national force from abroad against the Western powers during World War II.He was a contemporary of Mohandas K. Gandhi, at times an ally and at other ... Netaji Subhas Chandra Bose was born on 23 January,1897 in Cuttack, Orissa and he died on 18 August,1945, in a hospital in Taiwan after suffering from burn injuries sustained in a plane crash. Subhash Chandra Bose Birth. Subhash Chandra Bose was born on January 23, 1897, in Cuttack, Odisha, India. He was a prominent figure in India's independence movement against British rule. Subhash Chandra Bose's early education took place in Cuttack and later in Calcutta, where he developed a passion for nationalistic ideas. Subhash Chandra Bose, born on 23rd January 1897, in Cuttack, was the son of Janakinath Bose and Prabhavati Dutt. An iconic figure in the Indian independence movement, Bose emerged as a fearless revolutionary during the British colonial era. His unwavering courage and resolute patriotism elevated him to the status of a national hero, and Indians ...\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "output = search.invoke('Where Subash Chandra Bose born?')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9f23012-b164-4e8a-bd4a-d019caa43dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckduckgo_search'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b05362e-3db6-4387-93b6-16047808f9b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3385bb16-5046-4c0e-b14f-e18394787b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[snippet: Learn about the life and achievements of Utkalmani Gopabandhu Das, a social reformer, journalist, and freedom fighter from Odisha. He co-founded Utkala Sammilani, established Satyabadi School, and founded The Samaja newspaper., title: Utkalmani Gopabandhu Das - The Gem of Odisha, link: https://incredibleodisha.in/utkalmani-gopabandhu-das-the-gem-of-odisha/], [snippet: Learn about Gopabandhu Das, a visionary leader, social reformer, and nationalist who revolutionized education in Odisha. Discover his achievements, innovations, and challenges in establishing schools, colleges, and universities in the state., title: Gopabandhu Das : The National Education Planner of Odisha, link: https://www.odishalifestyle.com/gopabandhu-das-the-national-education/], [snippet: Gopabandhu Das came into this world on the 9th of October in the year 1877, in the serene village of Suando, situated in close proximity to Puri, within the picturesque state of Odisha. He was born into a Brahmin family, a heritage that would significantly influence his life's journey. Tragically, his entry into the world was marred by the ..., title: Gopabandhu Das Biography - starsunzip.com, link: https://starsunzip.com/gopabandhu-das/], [snippet: Gopabandhu Das, widely known as Utkalamani, was a multifaceted personality who left an indelible mark on the social, political, literary, and educational landscape of Odisha, then British India.Born on October 9, 1877, in Suando village near Puri, Odisha, he emerged as a social worker, reformer, political activist, journalist, poet, and essayist during a time of immense societal transformation., title: Utkalmani Gopabandhu Das Biography, Education, Religion, Death, link: https://phyinor.com/gopabandhu-das/]\n"
     ]
    }
   ],
   "source": [
    "search = DuckDuckGoSearchResults()\n",
    "output = search.run('Gopabandhu Das')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09f1cc95-1316-4f51-ae66-b2ef33b28453",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[snippet: Learn about the history, significance, and rituals of the Jagannath Temple, one of the four Char Dham pilgrimage sites in India. Discover the legends, myths, and culture of Lord Jagannath and his siblings, who are worshipped as manifestations of Lord Vishnu., title: Shree Jagannath Puri Temple - A Complete Visitor's Guide, link: https://www.godearlife.com/jagannath-puri/], [snippet: The Jagannath temple in Puri, Odisha is particularly significant in Vaishnavism, and is regarded as one of the Char Dham pilgrimage sites in India. The Jagannath temple is massive, over 61 metres (200 ft) high in the Nagara architecture style of Hindu temple architecture , and one of the best surviving specimens of Kalinga architecture , namely ..., title: Jagannath - Wikipedia, link: https://en.wikipedia.org/wiki/Jagannath], [snippet: Know the timing for Puri Jagannath Temple Darshan and save time with our guide. Bus, Train, Cab rates and tips for easy darshan. Know the best times and important days to visit Puri Jagannath Temple and the timing chart for every day. Plan your visit properly so that you can completely enjoy and have a pleasant Darshan of this Holy Dham., title: Puri Jagannath Temple darshan timings, Best time to visit, link: https://purijagannath.in/temple-darshan-timings/], [snippet: Learn about the ancient and sacred temple dedicated to Lord Jagannath, one of the four Dhams in India. Discover its history, architecture, rituals, darshan timings, and fascinating facts and mysteries., title: Shree Jagannath Temple In Puri Darshan Timings, History, Facts, Photos ..., link: https://incredibleodisha.in/jagannath-temple-in-puri/]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region='India', max_results=3, safesearch='moderate')\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
    "output = search.run('Jagannath Dham')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ae7cd9d-4f81-4f83-b9b4-50659c215815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snippet: Learn about the history, significance, and rituals of the Jagannath Temple, one of the four Char Dham pilgrimage sites in India. Discover the legends, myths, and culture of Lord Jagannath and his siblings, who are worshipped as manifestations of Lord Vishnu.\n",
      "Title: Shree Jagannath Puri Temple - A Complete Visitor's Guide\n",
      "Link: https://www.godearlife.com/jagannath-puri/\n",
      "\n",
      "__________________________________________________\n",
      "\n",
      "snippet: The Jagannath temple in Puri, Odisha is particularly significant in Vaishnavism, and is regarded as one of the Char Dham pilgrimage sites in India. The Jagannath temple is massive, over 61 metres (200 ft) high in the Nagara architecture style of Hindu temple architecture , and one of the best surviving specimens of Kalinga architecture , namely ...\n",
      "Title: Jagannath - Wikipedia\n",
      "Link: https://en.wikipedia.org/wiki/Jagannath\n",
      "\n",
      "__________________________________________________\n",
      "\n",
      "snippet: Know the timing for Puri Jagannath Temple Darshan and save time with our guide. Bus, Train, Cab rates and tips for easy darshan. Know the best times and important days to visit Puri Jagannath Temple and the timing chart for every day. Plan your visit properly so that you can completely enjoy and have a pleasant Darshan of this Holy Dham.\n",
      "Title: Puri Jagannath Temple darshan timings, Best time to visit\n",
      "Link: https://purijagannath.in/temple-darshan-timings/\n",
      "\n",
      "__________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'snippet: (.*?), title: (.*?), link: (.*?)\\],'\n",
    "matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "for snippet, title, link in matches:\n",
    "    print(f'snippet: {snippet}\\nTitle: {title}\\nLink: {link}\\n')\n",
    "    print('_'*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4b74f03-e33a-46fb-a34b-de26ed83f9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9858d546-7377-42c3-bd4b-60caec57dd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a217588a-42c9-4c65-b4d3-e9d27636455e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: NebulaGraph\\nSummary: NebulaGraph is an open-source distributed graph database built for super large-scale graphs with milliseconds of latency. NebulaGraph adopts the Apache 2.0 license and  also  comes with a wide range of data visualization tools.\\n\\n\\n\\nPage: Vector database\\nSummary: A vector database, vector store or vector search engine is a database that can store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more Approximate Nearest Neighbor (ANN) algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\\nVectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector\\'s position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\\nVector databases can be used for similarity search, multi-modal search, recommendations engines, large language models (LLMs), etc.\\nVector databases are also often used to implement Retrieval-Augmented Generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.\\n\\nPage: Prompt engineering\\nSummary: Prompt engineering is the process of structuring an instruction that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.\\nA prompt for a text-to-text language model can be a query such as \"what is Fermat\\'s little theorem?\", a command such as \"write a poem about leaves falling\", or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as asking the model to complete \"maison  house, chat  cat, chien \" (the expected response being dog), an approach called few-shot learning.\\nWhen communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper = WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=5000)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wiki.invoke({'query':'llamaindex'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ad1bd9e-206f-43f7-9db1-46d317ac8cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Page: Gemini (chatbot)\\nSummary: Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name and developed as a direct response to the meteoric rise of OpenAI\\'s ChatGPT, it was launched in a limited capacity in March 2023 before expanding to other countries in May. It was previously based on PaLM, and initially the LaMDA family of large language models.\\nLaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI\\'s launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard and sent them into a panic, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in February 2023, which took center stage during the 2023 Google I/O keynote in May and was upgraded to the Gemini LLM in December. Bard and Duet AI were unified under the Gemini brand in February 2024, coinciding with the launch of an Android app, which would replace Google Assistant as the main virtual assistant on Android, Google Assistant, however, will stay as an optional assistant.\\nGemini has received lukewarm responses. It became the center of controversy in February 2024, when social media users reported that it was generating historically inaccurate images of historical figures as people of color, with conservative commentators decrying its alleged bias as \"wokeness\".\\n\\nPage: Gemini (language model)\\nSummary: Google Gemini is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI\\'s GPT-4. It powers the chatbot of the same name.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.invoke({'query':'Google gemini'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff052a6-e980-4661-92ca-ae3ee49c30f8",
   "metadata": {},
   "source": [
    "### Creating a ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f3e0357-537d-4669-aa8b-28e677ac2370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchainhub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f02a8f44-7e6a-4d37-93bf-b9147d7b1151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "680e1aef-2dc7-4fe4-bc0b-0405b6ca13ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "['agent_scratchpad', 'input', 'tool_names', 'tools']\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=0)\n",
    "\n",
    "template = '''\n",
    "Answer the following questions as best you can.\n",
    "Questions: {q}\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt = hub.pull('hwchase17/react')\n",
    "print(type(prompt))\n",
    "print(prompt.input_variables)\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ce7f23f-b1ff-42ad-8f3d-647ca632c5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Python REPL tool (for executing python code)\n",
    "python_repl = PythonREPLTool()\n",
    "python_repl_tool = Tool(\n",
    "    name='Python REPL',\n",
    "    func=python_repl.run,\n",
    "    description='Useful when you wish to use Python to answer the question. You should input Python code'\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Wikipedia tool (for searching wikipedia)\n",
    "api_wrapper = WikipediaAPIWrapper()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wikipedia_tool = Tool(\n",
    "    name='Wikipedia',\n",
    "    func=wikipedia.run,\n",
    "    description='Useful when you wish to search a topic in wikipedia'\n",
    ")\n",
    "\n",
    "# 3.Duckduckgo Search tool (for general web search)\n",
    "search = DuckDuckGoSearchRun()\n",
    "duckduckgo_tool = Tool(\n",
    "    name='DuckDuckGo Search',\n",
    "    func=search.run,\n",
    "    description='Useful when you wish to search a topic in web'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3f47556e-d637-4e41-b652-a4285d6afe42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [python_repl_tool, wikipedia_tool, duckduckgo_tool]\n",
    "agent = create_react_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2206867a-33e0-4a34-bb08-5c14bf6c8c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5f798d4a-4d86-472d-a1a9-f2db006bf80c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can write a Python script to generate the first 20 numbers in the Fibonacci series.\n",
      "\n",
      "Action: Python REPL\n",
      "Action Input: \n",
      "```python\n",
      "def fibonacci(n):\n",
      "    fib_series = [0, 1]\n",
      "    for i in range(2, n):\n",
      "        next_fib = fib_series[-1] + fib_series[-2]\n",
      "        fib_series.append(next_fib)\n",
      "    return fib_series[:n]\n",
      "\n",
      "print(fibonacci(20))\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3m[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181]\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mI now know the first 20 numbers in the Fibonacci series.\n",
      "\n",
      "Final Answer: The first 20 numbers in the Fibonacci series are: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = 'Generate the first 20 numbers in the Fibonacci series'\n",
    "output = agent_executor.invoke({\n",
    "    'input':prompt_template.format(q=question)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "46a1e2e3-0adb-4734-97bd-76f6c4dae38c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the following questions as best you can.\n",
      "Questions: Generate the first 20 numbers in the Fibonacci series\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9bdb774-d39f-4ac1-ab59-3bff5ebc1c10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 20 numbers in the Fibonacci series are: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181.\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc9b2687-0d74-45c9-81c6-092c8513a006",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find the current chief minister of Odisha, which is a state in India. This information can be quickly found through a web search.\n",
      "\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: current chief minister of Odisha\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mOdisha Full Cabinet Ministers List 2024: A total of 15 ministers along with Odisha CM Mohan Charan Majhi were inducted into the Cabinet on Wednesday (June 12). BJP MLA from Keonjhar Mohan Charan Majhi, who belongs to the tribal Santhal community took oath as Odisha's Chief Minister. The party picked two Deputy CMs  K V Singh Deo and Pravati Parida, who became the first woman Deputy CM of ... In Short. Four-time MLA and tribal leader Mohan Charan Majhi on Wednesday took oath as BJP's first chief minister of Odisha. Governor Raghubar Das administered the oath to Majhi in an event that was attended by Prime Minister Narendra Modi and former Chief Minister Naveen Patnaik. Apart from PM Modi, Union Ministers JP Nadda, Amit Shah ... Mohan Charan Majhi was sworn in today as the BJP's first chief minister in Odisha. Majhi took the oath of office at Bhubaneswar's Janata Maidan today with a slew of dignitaries in attendance including Prime Minister Narendra Modi and Amit Shah. Majhi earlier met ex-chief minister Naveen Patnaik and invited him to the swearing-in ceremony. Mohan Charan Majhi, four-time MLA and tribal leader, will take the oath of office as Chief Minister of Odisha on Wednesday, June 121, 2024. The BJP on Tuesday, June 11, staked claim to form a ... Four-time MLA and tribal leader Mohan Charan Majhi takes oath as the first BJP Chief Minister of Odisha at a ceremony in Bhubaneswar on June 12, 2024.\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The current chief minister of Odisha is Mohan Charan Majhi.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = 'Who is the current chief minister of Odisha'\n",
    "output = agent_executor.invoke({\n",
    "    'input':prompt_template.format(q=question)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28161813-4f81-4e6a-b773-65bc2974a736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo provide information about Swami Vivekananda's early life, I should look up a reliable source that details his biography, such as Wikipedia.\n",
      "\n",
      "Action: Wikipedia\n",
      "Action Input: Swami Vivekananda\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Swami Vivekananda\n",
      "Summary: Swami Vivekananda (; Bengali: [ami bibekanndo] ; IAST: Svm Viveknanda ; 12 January 1863  4 July 1902), born Narendranath Datta (Bengali: [nrendronat dto]), was an Indian Hindu monk, philosopher, author, religious teacher, and the chief disciple of the Indian mystic Ramakrishna. He was a key figure in the introduction of Vedanta and Yoga to the Western world, and is the father of modern Indian nationalism who is credited with raising interfaith awareness and bringing Hinduism to the status of a major world religion in the late nineteenth century.\n",
      "Born into an aristocratic Bengali Kayastha family in Calcutta, Vivekananda was inclined from a young age towards religion and spirituality. He later found his guru Ramakrishna and became a monk. After the death of Ramakrishna, Vivekananda extensively toured the Indian subcontinent as a wandering monk and acquired first-hand knowledge of the living conditions of Indian people in then British India. Moved by their plight, he resolved to help them and found a way to travel to the United States, where he became a popular figure after the 1893 Parliament of Religions in Chicago at which he delivered his famous speech beginning with the words: \"Sisters and brothers of America ...\" while introducing Hinduism to Americans. He made such an impression there that an American newspaper described him as \"an orator by divine right and undoubtedly the greatest figure at the Parliament\".\n",
      "After great success at the Parliament, in the subsequent years, Vivekananda delivered hundreds of lectures across the United States, England, and Europe, disseminating the core tenets of Hindu philosophy, and founded the Vedanta Society of New York and the Vedanta Society of San Francisco (now Vedanta Society of Northern California), both of which became the foundations for Vedanta Societies in the West. In India, he founded the Ramakrishna Math, which provides spiritual training for monastics and householders, and the Ramakrishna Mission, which provides charity, social work and education.\n",
      "Vivekananda was one of the most influential philosophers and social reformers in his contemporary India, and the most successful missionary of Vedanta to the Western world. He was also a major force in contemporary Hindu reform movements and contributed to the concept of nationalism in colonial India. He is now widely regarded as one of the most influential people of modern India and a patriotic saint. His birthday in India is celebrated as National Youth Day.\n",
      "\n",
      "Page: Bibliography of Swami Vivekananda\n",
      "Summary: Swami Vivekananda (18631902) was an Indian Hindu monk and a key figure in the introduction of Indian philosophies of Vedanta and Yoga to the western world. He was one of the most influential philosophers and social reformers in his contemporary India and the most successful and influential missionaries of Vedanta to the Western world. Indian Nobel laureate poet Rabindranath Tagore's suggested to study the works of Vivekananda to understand India. He also told, in Vivekananda there was nothing negative, but everything positive.\n",
      "In last one century, hundreds of scholarly books have been written on Vivekananda, his works and his philosophy in different languages. Sister Nivedita, who was a disciple and a friend of Vivekananda, wrote two books The Master as I Saw Him and Notes of some wanderings with the Swami Vivekananda. The first one was published in 1910 and the second one was published in 1913. Sister Gargi's lifelong research work, a series of six volumes of books, Swami Vivekananda in the West: New Discoveries was first published in two volumes in 1957. In 198387, these series was republished in six volumes. Bengali scholar and critic Sankari Prasad Basu, who was a director of Swami Vivekananda Archives, Ramakrishna Mission Institute of Culture wrote several books on Vivekananda such as Vivekananda o Samakalin Bharatbarsha ((in Bengali) 7 volumes), Sahasya Vivekananda (in Bengali),\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "\n",
      "Final Answer: Swami Vivekananda, born Narendranath Datta on 12 January 1863 in Calcutta, was a prominent Indian Hindu monk, philosopher, and the chief disciple of the mystic Ramakrishna. Coming from an aristocratic Bengali Kayastha family, he showed an early inclination towards spirituality and religion. After meeting his guru, Ramakrishna, Vivekananda became a monk and, following Ramakrishna's death, traveled extensively across the Indian subcontinent as a wandering monk. His journey led him to deeply understand the conditions of the Indian people under British rule, motivating him to seek ways to help them. Vivekananda gained international fame after his speech at the 1893 Parliament of Religions in Chicago, where he introduced Hinduism to the Western world. He spent years lecturing in the United States, England, and Europe, founded the Vedanta Societies in New York and San Francisco, and back in India, established the Ramakrishna Math and Mission. Vivekananda's teachings and works made him one of the most influential philosophers and social reformers of his time, both in India and abroad.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = 'Tell me about Swami Vivekanand early life'\n",
    "output = agent_executor.invoke({\n",
    "    'input':prompt_template.format(q=question)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af546ab1-8256-4841-8134-9823fe301e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9dcced-8b91-426b-9cb4-29ef876b8167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37033da0-fad4-4a69-9b37-d25847e1f7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b6445d-6149-4c15-b830-30c9cae83d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
