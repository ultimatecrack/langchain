{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0558942-66a3-4cf3-85da-41c6950a60c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f085bdcc-9dca-4901-b8e0-f128412ac441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f348aa-d88f-4ac5-b060-cd7c0fca614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "661d9d0f-c495-4f91-9cba-7648151380a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "660e67d5-5bce-4674-97c4-abff595fe4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    print('Loading a file...')\n",
    "    if extension == '.pdf':\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document formant is not supported!')\n",
    "        return None\n",
    "        \n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49adc4-4ab4-40ed-906f-4ad9063a1bb6",
   "metadata": {},
   "source": [
    "### Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c427670-c7a4-45e1-bb79-a5fde5fc2630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a file...\n",
      "“Rich Dad Poor Dad  is a starting point for anyone looking to  \n",
      "gain control of their financial future.”\n",
      "     – USA TODAY\n"
     ]
    }
   ],
   "source": [
    "data = load_document('documents/Rich-Dad-Poor-Dad.pdf')\n",
    "print(data[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2575e83-8aea-406b-8d1b-8c054d7d1fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'documents/Rich-Dad-Poor-Dad.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "print(data[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24365bdc-ba31-49a1-812d-9c9f3d2defa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 241\n"
     ]
    }
   ],
   "source": [
    "print('Total pages: {}'.format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fde8230-a130-40af-a40d-d429ed0e9d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in the page: 121\n"
     ]
    }
   ],
   "source": [
    "print('Characters in the page: {}'.format(len(data[1].page_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8bff6a-be7b-4007-b844-61f0e0f8aa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a file...\n",
      "Rakesh Panigrahy | rakesh.eml42@iiitb.net | +91 9768807119 \n",
      "\n",
      "https://www.linkedin.com/in/rakesh-panigrahy-062166102/\n",
      "\n",
      "\n",
      "\n",
      "Summary:\n",
      "\n",
      "Highly skilled Data Scientist with 4+ years of experience in software development, analytics, and model building. Proven ability to leverage expertise in Machine Learning, Deep Learning, and NLP to build and deploy data-driven models for business process optimization and cost reduction.\n",
      "\n",
      "\n",
      "\n",
      "Skills:\n",
      "\n",
      "Machine Learning: ML, Deep Learning, NLP (Topic Modeling, NER, Transformers, Rasa), Computer Vision(YOLO, Pyteserract, GCV).\n",
      "\n",
      "Programming: Python, Django, SQL, AWS, Linux, Docker, Tensforflow, Pytorch, PHP, JS, Flask\n",
      "\n",
      "Data Visualization: Tableau, Metabase, MSTR\n",
      "\n",
      "Tools & Frameworks: MLflow, Pycaret, Airflow, Evidently, Streamlit, Sagemaker Studio, AWS, EC2, S3, GCP, CI/CD (Github action + Terraform), Cloud Build, Vertex AI\n",
      "\n",
      "LLM: GPT, BERT, Multimodal model finetuning using PEFT, LORA, QLORA, BitsandBytes\n",
      "\n",
      "\n",
      "\n",
      "Experience:\n",
      "\n",
      "PharmEasy Pvt Ltd | Mumbai, India | May 2021 - Present\n",
      "\n",
      "\n",
      "\n",
      "Business Analyst\n",
      "\n",
      "\n",
      "\n",
      "Developed dashboards and ML (Vision + NLP + Recommender) models to track KPIs, facilitated process migration and improvement.\n",
      "\n",
      "Built and deployed models for:\n",
      "\n",
      "Route Optimization Engine for B2B delivery (saved ₹30-40 Lac per year)\n",
      "\n",
      "Pharmeasy Lab Test Search Recommendation using probabilistic model.\n",
      "\n",
      "Visual KYC System for onboarding retailers to retailer master using drug license.\n",
      "\n",
      "Department wise customer auto issue assignment (Topic Modelling with Non-Negative Matrix Factorization).\n",
      "\n",
      "Visual text extraction of SIP-TIP supplier invoices using OCR. \n",
      "\n",
      "\n",
      "\n",
      "Cogno AI | Mumbai, India | Dec 2020 - May 2021\n",
      "\n",
      "\n",
      "\n",
      "Django Developer\n",
      "\n",
      "Chatbot data annotation, custom logic (API + report) development, training and integration with Django application, deployment, and version migration on AWS EC2.\n",
      "\n",
      "\n",
      "\n",
      "Neural IT Pvt. Ltd. | Mumbai, India | Aug 2019 - Dec 2020\n",
      "\n",
      "\n",
      "\n",
      "Software Developer\n",
      "\n",
      "Contributed to core development team activities including HR management application development, code review, bug fixing, workflow management, and issue analysis.\n",
      "\n",
      "\n",
      "\n",
      "Education:\n",
      "\n",
      "M.Sc. - Machine Learning & AI | Liverpool John Moores University | Liverpool, UK | June 2022 - June 2024\n",
      "\n",
      "Executive Post Graduate Programme in Machine Learning and Artificial Intelligence | IIIT Bangalore | Bangalore | June 2022 - August 2023\n",
      "\n",
      "B.Sc. - Information Technology | Model College Thakurli, Mumbai University | Mumbai, India | June 2016 - June 2019\n",
      "\n",
      "\n",
      "\n",
      "Additional Information:\n",
      "\n",
      "There are other projects, I worked on as part of my academic task and those are Image captioning (Using attention), Gesture recognition (Conv3D and Conv2D+LSTM), Disease and Treatment detection (NER). Currently I am researching on Visual Question Answering Multimodal.\n"
     ]
    }
   ],
   "source": [
    "data = load_document('documents/Rakesh Panigrahy - Data Science:ML.docx')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc091c-3fac-44b3-9b83-6b88517dd9ae",
   "metadata": {},
   "source": [
    "### Load from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dfe0fc8-12c3-4083-95c6-d29f280a2e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd0fc60a-814a-4a8a-a7aa-4fcc599ea4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88acf9d2-b879-49c7-956b-8f07e02e79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ପୃଥିବୀ ବର୍ଷକୁ ଥରେ ସୂର୍ଯ୍ୟଙ୍କର ଚାରିଆଡ଼େ ଘୁରି ଆସେ, କିନ୍ତୁ ଜ୍ୟୋତିଷ ଗଣନାର ସୁବିଧା ପାଇଁ ପଣ୍ଡିତମାନେ କଳ୍ପନା କରିଅଛନ୍ତି ଯେ ସମୁଦାୟ ଆକାଶ ବା ଖ-ଗୋଳରେ ଥାଇ ସୂର୍ଯ୍ୟ ୧୨ ମାସ (୩୬୫ ଦିନ ବା ଏକ ସୌର ବର୍ଷ)ରେ ପୃଥିବୀର ଚାରିଆଡ଼େ ଥରେ ଘୁରି ଆସନ୍ତି ।  ବର୍ଷକ ୧୨ ମାସ ଥିବାରୁ ଖ-ଗୋଳ (୩୬୦ ଡିଗ୍ରୀ)କୁ ୧୨ ଭାଗରେ ବିଭକ୍ତ କରା ଯାଇଅଛି ।  ଏହି ପ୍ରତ୍ୟେକ ଭାଗ ୩୦ ଡିଗ୍ରୀ ଅଟେ ଓ ପ୍ରତ୍ୟେକ ୩୦ ଡିଗ୍ରୀ ପରିମିତ ସୀମା ମଧ୍ୟରେ ଦେଖା ଯାଉଥିବା କେତେକ ଉଜ୍ଜଳ ନକ୍ଷତ୍ରମାନଙ୍କୁ ଯୋଗ କରି ଗୋଟିଏ ଗୋଟିଏ ଜୀବ (ଯଥା- ମେଷ, ବୃଷ, ମିଥୁନ, କକଡ଼ା, ସିଂହ, କନ୍ୟା, ବିଛା, ମକର, ମୀନ)ର ବା ବସ୍ତୁ (ତୁଳାଯନ୍ତ୍ର, ଧନୁ, କୁମ୍ଭ)ର ଛବି କଳ୍ପନା ସାହାଯ୍ୟରେ ଅଙ୍କିତ କରାଯାଇ ସେହି ନକ୍ଷତ୍ରମାନଙ୍କୁ ସେହି ଜୀବ ବା ବସ୍ତୁ ନାମରେ ଡକାଯାଉଅଛି । ଉଦାହରଣସ୍ୱରୁପ ଦେଖାଯିବ ଯେ ଯେଉଁ ୩୦ ଡିଗ୍ରୀ ସୀମା ମଧ୍ୟରେ ବିଛା ରାଶି ଅବସ୍ଥିତ ସେଥିରେ ଥିବା ଉଜ୍ଜଳ ନକ୍ଷତ୍ରଗୁଡ଼ିକୁ ଯୋଗ କଲେ ଗୋଟିଏ କଙ୍କଡ଼ା ବିଛାର ରୂପ କଳ୍ପିତ ହୋଇପାରିବ ।  ଏଥିପାଇଁ ଉକ୍ତ ୩୦ ଡିଗ୍ରୀକୁ ବିଛାରାଶି ନାମରେ ଡକାଯାଏ ।  ଚନ୍ଦ୍ର ପୃଥିବୀର ଚାରିଆଡ଼େ ୩୦ଦିନରେ ଥରେ ଘୁରି ଆସନ୍ତି।  ଏଥିପାଇଁ ଚାନ୍ଦ୍ରମାନ ମାସକ ୩୦ ଦିନ ଓ ଚାନ୍ଦ୍ରମାନ ବର୍ଷକ ୩୬୦ଦିନ ।\n",
      "ପ୍ରତ୍ୟେକ ନକ୍ଷତ୍ର ପୃଥିବୀର ଚାରିଆଡ଼େ ୨୭ଦିନରେ ଥରେ ଘୁରି ଆସେ । ଏଥିପାଇଁ ନାକ୍ଷତ୍ରିକ ମାସ ୨୭ଦିନ । ନକ୍ଷତ୍ରମାନଙ୍କୁ ହିନ୍ଦୁ ଜ୍ୟୋତିଷମାନେ ୨୭ଟି ସ୍ୱତନ୍ତ୍ର ତାରକାପୁଞ୍ଜରେ ବିଭକ୍ତକରି ପ୍ରତ୍ୟେକଙ୍କୁ ଗୋଟିଏ ଲେଖାଁଏ ସ୍ୱତନ୍ତ୍ର ନାମ ଦେଇ ଅଛନ୍ତି । ସୂର୍ଯ୍ୟ ଓ ଚନ୍ଦ୍ରଙ୍କ ପରି ନକ୍ଷତ୍ରମାନେ ପୂର୍ବରେ ଉଦିତ ହୋଇ ପଶ୍ଚିମରେ ଅସ୍ତ ଯାଆନ୍ତି । ସବୁ ନକ୍ଷତ୍ର ଆକାଶର ଉତ୍ତରଦିଗରେ ଥିବା ଧ୍ରୁବ ନକ୍ଷତ୍ରକୁ କେନ୍ଦ୍ର କରି ତାହାର ଚାରିଆଡ଼େ ପୂର୍ବରୁ ପଶ୍ଚିମକୁ ବୁଲୁଅଛନ୍ତି । ଧ୍ରୁବନକ୍ଷତ୍ର ପୃଥିବୀର ଉତ୍ତରମେରୁର ଠିକ ମୁଣ୍ଡ ଉପରେ ଆକାଶରେ ଅବସ୍ଥିତ । ଏହା ସର୍ବଦା ଆକାଶରେ ଏକ ସ୍ଥାନରେ ସ୍ଥିର ହୋଇଥାଏ, ଏହାର ଉଦୟ ବା ଅସ୍ତ ନାହିଁ । ନାବିକମାନେ ଓ ମରୁଭୂମିରେ ଭୁଆଁବୁଲିବା ବାଟୋଇମାନେ ଏହି ଧ୍ରୁବନକ୍ଷତ୍ରକୁ ଦେଖି ଅକୂଳ ସମୁଦ୍ରରେ ଓ ଅପନ୍ତରା ମରୁଭୂମିରେ ରାତ୍ରିରେ ଦିଗ ନିର୍ଣ୍ଣୟ କରନ୍ତି । ଏ ଧ୍ରୁବ ନକ୍ଷତ୍ରକୁ ଚିହ୍ନିବା ସହଜ ଅଟେ । ଉତ୍ତରାକାଶରେ ସପ୍ତର୍ଷିମଣ୍ଡଳ ନାମକ ୭ଟି ଉଜ୍ଜଳ ନକ୍ଷତ୍ରବିଶିଷ୍ଟ ଗୋଟିଏ ନକ୍ଷତ୍ର ପୁଞ୍ଜ ଅଛି; ଏହି ମଣ୍ଡଳର ମୁଣ୍ଡପାଖର ୨ଟି ନକ୍ଷତ୍ର ଯୋଗ କରି ସେହି ଗାରକୁ ବଢ଼ାଇଦେଲେ ତାହା ଧ୍ରୁବ ତାରା ପାଖ ଦେଇଯିବ ।  ଥରେ ଧ୍ରୁବତାରାକୁ ଓ ସପ୍ତର୍ଷିମଣ୍ଡଳକୁ ଚିହ୍ନି ରଖିଲେ ତାରାମାନଙ୍କୁ ଚିହ୍ନିବା ସହଜ ହୁଏ । ପ୍ରତିଦିନ ପ୍ରତି ନକ୍ଷତ୍ର ପୂର୍ବଦିନ ଅପେକ୍ଷା ୪ ମିନିଟ ଲେଖାଏଁ ଡେରି କରି ଉଦିତ ହୁଏ; ଅର୍ଥାତ ଆଜି ଯେଉଁ ତାରା ସନ୍ଧ୍ୟା ୭ଟାବେଳେ ଉଇଁଛି କାଲି ସେ ତାରା ଉଇଁଲାବେଳକୁ ୭ଟା ବାଜି ୪ ମିନିଟ ହୋଇଥିବ । ସୂର୍ଯ୍ୟ ଯେଉଁ ରାଶିରେ ଯେଉଁ ମାସରେ ଅବସ୍ଥାନ କରୁଥାଆନ୍ତି ସୌରମାସର ନାମକରଣ ସେହି ରାଶିର ନାମ ଅନୁସାରେ ହୁଏ । ଯେଉଁ ଚାନ୍ଦ୍ରମାସର ପୂର୍ଣ୍ଣିମାର ସନ୍ଧ୍ୟାରେ ଚନ୍ଦ୍ର ଯେଉଁ ନକ୍ଷତ୍ର ସଙ୍ଗେ ଉଦିତ ହୁଅନ୍ତି ସେହି ନକ୍ଷତ୍ରର ନାମ ଅନୁସାରେ ତାହା ପୂର୍ବର ଚାନ୍ଦ୍ରମାସର ନାମକରଣ ହୁଏ; ଯଥା ବୈଶାଖ ପୂର୍ଣ୍ଣିମାରେ ଚନ୍ଦ୍ର ବିଶାଖା ନକ୍ଷତ୍ର ସଙ୍ଗେ ଉଦିତ ହୁଅନ୍ତି ବୋଲି ପୂର୍ବବର୍ତ୍ତୀ ମାସର ନାମ ବୈଶାଖ । ଆକାଶରେ ନକ୍ଷତ୍ର ଓ ନକ୍ଷତ୍ରପୁଞ୍ଜମାନଙ୍କ ଅବସ୍ଥିତିମାନଙ୍କୁ ଧ୍ରୁବତାରାର ଅବସ୍ଥିତି ସଙ୍ଗେ ଓ ପରସ୍ପରର ଅବସ୍ଥିତି ସଙ୍ଗେ ତୁଳନା କଲେ ଧ୍ରୁବର ଅବସ୍ଥିତି ବଦଳୁନଥିବା ବୋଲି ଜଣାଯାଏ ।\n",
      "ପ୍ରାଚୀନ ହିନ୍ଦୁମାନେ ଚନ୍ଦ୍ରର ଗତିପଥର ସୀମା ମଧ୍ୟରେ ଥିବା ଭିନ୍ନ ଭିନ୍ନ ନକ୍ଷତ୍ରମାନଙ୍କୁ ୨୭ଟି ନାମ ଦେଇଥିଲେ ଓ ଏ ୨୭ ନକ୍ଷତ୍ରରୁ ଅଧିକା ଅଭିଜିତ ନାମରେ ଆଉ ଗୁଡିଏ ନକ୍ଷତ୍ରକୁ ହିନ୍ଦୁମାନେ ନାମ ଦେଇଥିଲେ । ହିନ୍ଦୁମାନଙ୍କ ବାରଟି ରାଶିର ନାମ ସଙ୍ଗେ ପ୍ରାଚୀନ ମିଶର, ଆରବ, ଗ୍ରୀସ ଓ ରୋମବାସୀମାନଙ୍କ ଦିଆ ନାମର ସମାନତା ଅଛି । ଯଥା ହିନ୍ଦୁମାନଙ୍କ ସିଂହ ରାଶି = ଗ୍ରୀକମାନଙ୍କର ଲିଓ (Leo)= ସିଂହ; କିନ୍ତୁ ହିନ୍ଦୁମାନଙ୍କ ପ୍ରଦତ୍ତ ନକ୍ଷତ୍ରମାନଙ୍କ ନାମ ସଙ୍ଗେ ଗ୍ରୀକ ଆଦିଙ୍କ ପ୍ରଦତ୍ତ ନକ୍ଷତ୍ରମାନଙ୍କ ନାମର ସାମଞ୍ଜସ୍ୟ ଦେଖାଯାଏ ନାହିଁ ।  ହିନ୍ଦୁ ଜ୍ୟୋତିଷୋକ୍ତ ୨୭ନକ୍ଷତ୍ର ଯଥାକ୍ରମେ ପଶ୍ଚିମରୁ ପୂର୍ବଆଡ଼କୁ ଦେଖା ଯାଆନ୍ତି ।\n",
      "ଖଗୋଳ = ୪ ସମକୋଣ = ୩୬୦ ଡିଗ୍ରୀ ।  ଏହି ୩୬୦ ଡିଗ୍ରୀକୁ ୧୨ଟି ରାଶି ଏବଂ ୨୭ଟି ନକ୍ଷତ୍ର ଅଧିକାର କରିଅଛି ।  ପ୍ରତ୍ୟେକ ରାଶିର ଅଧିକୃତ ସ୍ଥାନ = ୩୬୦/୧୨ = ୩୦ ଡିଗ୍ରୀ, ଏବଂ ପ୍ରତ୍ୟେକ ନକ୍ଷତ୍ରର ଅଧିକୃତ ସ୍ଥାନ = ୩୬୦/୨୭ = ୧୩ ୧/୩ ଡିଗ୍ରୀ; ଅର୍ଥାତ ପ୍ରତ୍ୟେକ ରାଶିର ସୀମା ମଧ୍ୟରେ ୨ ୧/୪ ନକ୍ଷତ୍ର ଅଛି ।\n",
      "\n",
      "ମେଷ ରାଶି (La-Aries, En-The Ram) = ୧ ଅଶ୍ୱିନୀ ନକ୍ଷତ୍ର + ୧ ଦ୍ୱିଜା + ୧/୪ କୃତ୍ତିକା\n",
      "ବୃଷ (La-Taurus, En-The bull) = ୩/୪ କୃତ୍ତିକା + ୧ ରୋହିଣି + ୧/୨ ମୃଗଶିରା\n",
      "ମିଥୁନ (La-Gemini, En-The twins) = + ୧/୨ ମୃଗଶିରା + ୧ ଆର୍ଦ୍ରା + ୩/୪ ପୁନର୍ବସୁ\n",
      "କର୍କଟ (La-Cancer, En-The crab) = ୧/୪ ପୁନର୍ବସୁ+ ୧ ପୁଷ୍ୟା + ୧ ଅଶ୍ଳେଷା\n",
      "ସିଂହ (La-Leo, En-Lion) = ୧ ମଘା + ୧ ପୂର୍ବଫାଲ୍ଗୁନୀ + ୧/୪ ଉତ୍ତର ଫାଲ୍ଗୁନୀ\n",
      "କନ୍ୟା (La-Virgo, En-Virgin) = ୩/୪ ଉତ୍ତର ଫାଲ୍ଗୁନୀ + ୧ ହସ୍ତା + ୧ ଚିତ୍ରା\n",
      "ତୁଳା (La-Libra, En-The scales) = ୧/୨ ଚିତ୍ରା + ୧ ସ୍ୱାତି + ୩/୪ ବିଶାଖା\n",
      "ବିଛା (La-Scorpio, En-Scorpion) = ୧/୪ ବିଶାଖା + ୧ ଅନୁରାଧା + ୧ ଜ୍ୟେଷ୍ଠା\n",
      "ଧନୁ (La-Sagitarius, En-The archer) = ୧ ମୂଲା + ୧ ପୂର୍ବାଷାଢ଼ା + ୧/୪ ଉତ୍ତରାଷାଢ଼ା\n",
      "ମକର (La-Capricornus, En-Capricorn) = ୩/୪ ଉତ୍ତରାଷା\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('Gemini', lang='or')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3645004-8cb2-4cc2-baa9-9618c1a4bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def chunk_data(data, chunk_size=256):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=30)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a99546e5-a9bd-49fe-a2c8-a4f043046dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a file...\n"
     ]
    }
   ],
   "source": [
    "data = load_document('documents/Rakesh Panigrahy - Data Science:ML.docx')\n",
    "chunks = chunk_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99df79c3-96e0-43d3-a44d-10a9f5a82b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2fcd2ee-79c1-46f6-af0a-7e00c687d23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Software Developer\\n\\nContributed to core development team activities including HR management application development, code review, bug fixing, workflow management, and issue analysis.\\n\\n\\n\\nEducation:' metadata={'source': 'documents/Rakesh Panigrahy - Data Science:ML.docx'}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd59f42e-5e28-473c-b92d-09d6bdded375",
   "metadata": {},
   "source": [
    "### Check Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37276f89-63b2-46b7-a4c0-770124c7e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 661\n",
      "Embedding Cost in USD: 0.000264\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "def print_embedding_cost(texts):\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000*0.0004:.6f}')\n",
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdffdaa-42ca-4e72-b5af-0b9f1a1e8cbd",
   "metadata": {},
   "source": [
    "### Embedding and Uploading to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b72915d4-04ca-41ed-8f7a-ed966c62bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pinecone import PodSpec\n",
    "def insert_fetch_embeddings(index_name, chunks):\n",
    "    pc = pinecone.Pinecone()\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    \n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'{index_name} already exists. Loading embeddings...', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('OK')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings...', end='')\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=pinecone.PodSpec(\n",
    "            environment='gcp-starter'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        vector_store = Pinecone.from_documents(documents=chunks, embedding=embeddings, index_name=index_name)\n",
    "        print('OK')\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ba71f7f-f6fc-44a5-82ee-b3c269470a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    pc = pinecone.Pinecone()\n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes...')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('OK')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name}...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "811e5074-7101-4037-a560-f67a5832b88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1d15822-9f9d-4865-9f91-ca9d3a95d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askdocument already exists. Loading embeddings...OK\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askdocument'\n",
    "vector_store = insert_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fafac54-d97a-4778-bb1a-f9e1898f1248",
   "metadata": {},
   "source": [
    "### Asking & Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71fb635a-a947-44ba-b099-3e1a4d84838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def ask_questions(vector_store, q):\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "    answer = chain.run(q)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa49821c-f74a-42fa-b1be-8ca478b671ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the skill sets of the individual mentioned include:\n",
      "\n",
      "- Machine Learning (ML), Deep Learning, NLP (Topic Modeling, NER, Transformers, Rasa), Computer Vision (YOLO, Pyteserract, GCV).\n",
      "- Programming in Python, Django, SQL, AWS, Linux, Docker, Tensorflow, Pytorch, PHP, JS, Flask.\n"
     ]
    }
   ],
   "source": [
    "q = 'What are the skill sets you can see?'\n",
    "answer = ask_questions(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e15587b-4033-4cd6-958a-a82a2954f725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the projects you have worked on or are currently working on include:\n",
      "\n",
      "1. Image captioning (Using attention)\n",
      "2. Gesture recognition (Conv3D and Conv2D+LSTM)\n",
      "3. Disease and Treatment detection (NER)\n",
      "4. Visual Question\n",
      "5. Department wise customer auto issue assignment (Topic Modelling with Non-Negative Matrix Factorization)\n",
      "6. Visual text extraction of SIP-TIP supplier invoices using OCR\n",
      "\n",
      "Additionally, you have experience with Data Visualization tools like Tableau, Metabase, MSTR, and various Tools & Frameworks such as MLflow, Pycaret, Airflow, Evidently, Streamlit, Sagemaker Studio, AWS, EC2, S3, GCP, CI/CD (Github action + Terraform), Cloud Build, Vertex AI.\n"
     ]
    }
   ],
   "source": [
    "q = 'What are the projects you can see?'\n",
    "answer = ask_questions(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d41d5b2b-74f1-4451-850b-c9f3863faa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user's qualifications are: \n",
      "1. Executive Post Graduate Programme in Machine Learning and Artificial Intelligence from IIIT Bangalore.\n",
      "2. B.Sc. in Information Technology from Model College Thakurli, Mumbai University.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the qualification?'\n",
    "answer = ask_questions(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80f6313b-4f96-4547-97e6-da73328b54f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #1:  what is the contact details\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: You can reach out to Rakesh Panigrahy via email at rakesh.eml42@iiitb.net or by phone at +91 9768807119. Additionally, you can connect with him on LinkedIn at https://www.linkedin.com/in/rakesh-panigrahy-062166102/.\n",
      "\n",
      " --------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #2:  What are the companies he worked with and how many years of experience he has?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Rakesh Panigrahy has worked with PharmEasy Pvt Ltd in Mumbai, India since May 2021. His total experience is at least 4+ years based on the information provided.\n",
      "\n",
      " --------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #3:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting... bye bye !\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit', 'bye']:\n",
    "        print('Quitting... bye bye !')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer = ask_questions(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n {\"-\"*50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00492b72-9332-4c24-9996-d88eb87fe419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1db0d21b-eb24-4a57-b3a1-db04ae503820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index chatgpt and embeddings..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('ChatGPT', 'en')\n",
    "chunks = chunk_data(data)\n",
    "index_name = 'chatgpt'\n",
    "vector_store = insert_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2b932de-9cdb-4a88-b8fc-2840baf4d4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT was launched on November 30, 2022.\n"
     ]
    }
   ],
   "source": [
    "q = 'When chatgpt created?'\n",
    "answer = ask_questions(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "968bd01d-e31a-48a0-b937-03e675704511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 was launched on March 14, 2023, and made publicly available via the paid chatbot product ChatGPT.\n"
     ]
    }
   ],
   "source": [
    "q = 'When GPT-4 created?'\n",
    "answer = ask_questions(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0a6e4-1580-4af6-b9e1-4c14db0adedc",
   "metadata": {},
   "source": [
    "### Using Chroma as a Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38e9fc49-3c86-464a-976a-5fcb079a0c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-genai 1.0.7 requires langchain-core<0.3,>=0.2.9, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "271ad300-ede0-4148-b68f-0ddc4a149de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "656106a0-3555-4739-b013-4aa845cb6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7b58f6f9-16e3-4c06-8661-33c05f94a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "data = load_document('documents/Rich-Dad-Poor-Dad.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d436005b-6cdc-45fe-a349-03c357ec4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the author is Robert Kiyosaki.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the name of the author?'\n",
    "answer = ask_questions(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7653fc62-b7a1-4b0e-bc23-68ce99bed93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author has two fathers, one rich and one poor. Their names are not given in the context provided.\n"
     ]
    }
   ],
   "source": [
    "q = 'How many fathers he has and what are the names of his fathers?'\n",
    "answer = ask_questions(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cc428fd9-e2d3-4023-87f9-61b993be287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the book mentions six lessons:\n",
      "\n",
      "1. Lesson 1: Inspiring people to learn more.\n",
      "2. Lesson 2: Demonstrating that it is easy if the foundation is strong.\n",
      "3. Lesson 3: Showing that anyone can achieve great wealth.\n",
      "4. Lesson 2: Why Teach Financial Literacy?\n",
      "5. Lesson 3: Mind Your Own Business\n",
      "\n",
      "Unfortunately, the remaining lesson numbers and titles are not explicitly mentioned in the provided context.\n"
     ]
    }
   ],
   "source": [
    "q = '5 lessons that this books tells'\n",
    "answer = ask_questions(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d29249e2-ea1d-4b92-9cd4-208769fe667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author melted down the toothpaste tubes in a steel pot over coals and collected them in a cardboard box that had previously held catsup bottles.\n"
     ]
    }
   ],
   "source": [
    "q = 'What the auhor did with toothpaste packets?'\n",
    "answer = ask_questions(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9fde4397-6527-461b-bc39-0744cd084c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author in the provided context did not sell a flat himself, but rather shared experiences of purchases and investments in real estate with others. He highlighted the value of making a profit when buying a property rather than just waiting for its price to go up. He advised someone else to sell a house using a 1031 tax-deferred exchange and they were able to make a significant profit ($156,000), having bought it for a lower price earlier.\n"
     ]
    }
   ],
   "source": [
    "q = 'how author sold a flat and got good profit?'\n",
    "answer = ask_questions(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b04d2-5930-467b-84c9-c93a1673ff43",
   "metadata": {},
   "source": [
    "### Adding Memory (Chat History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9cb75703-add9-40da-99a9-91ca70c43f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':5})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6be298eb-861b-44fc-8427-98e13e4d90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_chroma(q, chain):\n",
    "    result = chain.invoke({'question':q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "26cf83f6-2318-45f1-9138-a4701e78aeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "data = load_document('documents/Rich-Dad-Poor-Dad.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb81226f-c00e-403d-840a-14dbe1bfdd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Rich Dad Poor Dad69\n",
      "It was pretty confusing at first, but after reading it, it began to make \n",
      "some sense: \n",
      "Wealth is a person’s ability to survive so many number of days \n",
      "forward—or, if I stopped working today, how long could I survive?\n",
      "\n",
      "Rich Dad Poor Dad69\n",
      "It was pretty confusing at first, but after reading it, it began to make \n",
      "some sense: \n",
      "Wealth is a person’s ability to survive so many number of days \n",
      "forward—or, if I stopped working today, how long could I survive?\n",
      "\n",
      "Rich Dad Poor Dad69\n",
      "It was pretty confusing at first, but after reading it, it began to make \n",
      "some sense: \n",
      "Wealth is a person’s ability to survive so many number of days \n",
      "forward—or, if I stopped working today, how long could I survive?\n",
      "\n",
      "have wealth?\n",
      "As well as having my own definitions for assets and liabilities,  \n",
      "I also have my own definition for wealth. Actually, I borrowed it from \n",
      "a man named R. Buckminster Fuller. Some call him a quack, and\n",
      "\n",
      "have wealth?\n",
      "As well as having my own definitions for assets and liabilities,  \n",
      "I also have my own definition for wealth. Actually, I borrowed it from \n",
      "a man named R. Buckminster Fuller. Some call him a quack, and\n",
      "Human: What was the net wealth of the author?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'What was the net wealth of the author?', 'chat_history': [HumanMessage(content='What was the net wealth of the author?'), AIMessage(content='I don\\'t have specific information on the net wealth of the author of \"Rich Dad Poor Dad.\"')], 'answer': 'I don\\'t have specific information on the net wealth of the author of \"Rich Dad Poor Dad.\"'}\n"
     ]
    }
   ],
   "source": [
    "q = 'What was the net wealth of the author?'\n",
    "answer = ask_question_chroma(q, crc)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f55f0d3e-b01c-4aff-ba47-7de1ae1f1557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What was the net wealth of the author?\n",
      "Assistant: I don't have specific information on the net wealth of the author of \"Rich Dad Poor Dad.\"\n",
      "Human: What was the name his friend?\n",
      "Assistant: I don't know.\n",
      "Follow Up Input: What were his fathers doing?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Rich Dad Poor Dad147\n",
      "he was so busy working on his career. I hated it when my dad worked \n",
      "hard all his life and the government took most of what he worked for \n",
      "at his death. He could not even pass on what he worked so hard for\n",
      "\n",
      "Rich Dad Poor Dad147\n",
      "he was so busy working on his career. I hated it when my dad worked \n",
      "hard all his life and the government took most of what he worked for \n",
      "at his death. He could not even pass on what he worked so hard for\n",
      "\n",
      "Rich Dad Poor Dad147\n",
      "he was so busy working on his career. I hated it when my dad worked \n",
      "hard all his life and the government took most of what he worked for \n",
      "at his death. He could not even pass on what he worked so hard for\n",
      "\n",
      "service, he hired his two sons to give him a hand on the night shift \n",
      "and on weekends. When his boys went off to college, he said to them, \n",
      "‘Hurry back because someday this business will belong to you.’\n",
      "\n",
      "service, he hired his two sons to give him a hand on the night shift \n",
      "and on weekends. When his boys went off to college, he said to them, \n",
      "‘Hurry back because someday this business will belong to you.’\n",
      "Human: What were his fathers doing?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'What were his fathers doing?', 'chat_history': [HumanMessage(content='What was the net wealth of the author?'), AIMessage(content='I don\\'t have specific information on the net wealth of the author of \"Rich Dad Poor Dad.\"'), HumanMessage(content='What was the name his friend?'), AIMessage(content=\"I don't know.\"), HumanMessage(content='What were his fathers doing?'), AIMessage(content='The father in the provided context was working hard on his career, and eventually, he hired his two sons to help him with his business.')], 'answer': 'The father in the provided context was working hard on his career, and eventually, he hired his two sons to help him with his business.'}\n"
     ]
    }
   ],
   "source": [
    "q = 'What were his fathers doing?'\n",
    "answer = ask_question_chroma(q, crc)\n",
    "print(answer['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3bec850a-a7c7-45a9-b024-1a43fc35a0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What was the net wealth of the author?\n",
      "Assistant: I don't have specific information on the net wealth of the author of \"Rich Dad Poor Dad.\"\n",
      "Human: What was the name his friend?\n",
      "Assistant: I don't know.\n",
      "Human: What were his fathers doing?\n",
      "Assistant: The father in the provided context was working hard on his career, and eventually, he hired his two sons to help him with his business.\n",
      "Follow Up Input: What were his poor father and rich father doing?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Having two dads offered me the choice of  \n",
      "contrasting points of view:  \n",
      "one of a rich man and one of a poor man.  Introduction\n",
      "RICH DAD POOR DAD1\n",
      "I had two fathers, a rich one and a poor one. One was highly\n",
      "\n",
      "Having two dads offered me the choice of  \n",
      "contrasting points of view:  \n",
      "one of a rich man and one of a poor man.  Introduction\n",
      "RICH DAD POOR DAD1\n",
      "I had two fathers, a rich one and a poor one. One was highly\n",
      "\n",
      "Having two dads offered me the choice of  \n",
      "contrasting points of view:  \n",
      "one of a rich man and one of a poor man.  Introduction\n",
      "RICH DAD POOR DAD1\n",
      "I had two fathers, a rich one and a poor one. One was highly\n",
      "\n",
      "my poor dad I would hear how the \n",
      "rich were greedy crooks who should be \n",
      "made to pay more taxes. Both sides had \n",
      "valid points. It was difficult to go to \n",
      "work for one of the biggest capitalists in \n",
      "town and come home to a father who\n",
      "\n",
      "my poor dad I would hear how the \n",
      "rich were greedy crooks who should be \n",
      "made to pay more taxes. Both sides had \n",
      "valid points. It was difficult to go to \n",
      "work for one of the biggest capitalists in \n",
      "town and come home to a father who\n",
      "Human: What were his poor father and rich father doing?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The poor father believed that the rich were greedy crooks who should pay more taxes, while the rich father was likely a successful capitalist or businessman.\n"
     ]
    }
   ],
   "source": [
    "q = 'What were his poor father and rich father doing?'\n",
    "answer = ask_question_chroma(q, crc)\n",
    "print(answer['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d06c72e-3896-4320-801e-bde7dc1bb430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What was the net wealth of the author?\n",
      "Assistant: I don't have specific information on the net wealth of the author of \"Rich Dad Poor Dad.\"\n",
      "Human: What was the name his friend?\n",
      "Assistant: I don't know.\n",
      "Human: What were his fathers doing?\n",
      "Assistant: The father in the provided context was working hard on his career, and eventually, he hired his two sons to help him with his business.\n",
      "Human: What were his poor father and rich father doing?\n",
      "Assistant: The poor father believed that the rich were greedy crooks who should pay more taxes, while the rich father was likely a successful capitalist or businessman.\n",
      "Follow Up Input: What was my previous question?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "or I would ask our teachers how what we studied was applicable in \n",
      "the real world, or why we never studied money and how it worked. \n",
      "To the latter question, we often got the answer that money was not\n",
      "\n",
      "or I would ask our teachers how what we studied was applicable in \n",
      "the real world, or why we never studied money and how it worked. \n",
      "To the latter question, we often got the answer that money was not\n",
      "\n",
      "or I would ask our teachers how what we studied was applicable in \n",
      "the real world, or why we never studied money and how it worked. \n",
      "To the latter question, we often got the answer that money was not\n",
      "\n",
      "Those questions led me to become a student of education outside \n",
      "the hallowed walls of the school system. The more I studied, the more \n",
      "I understood why I did not like school and why schools failed to serve \n",
      "most of its students, even the “A” students.\n",
      "\n",
      "Those questions led me to become a student of education outside \n",
      "the hallowed walls of the school system. The more I studied, the more \n",
      "I understood why I did not like school and why schools failed to serve \n",
      "most of its students, even the “A” students.\n",
      "Human: What was your previous question?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The previous question was about why the author never studied money and how it worked in school.\n"
     ]
    }
   ],
   "source": [
    "q = 'What was my previous question?'\n",
    "answer = ask_question_chroma(q, crc)\n",
    "print(answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8769cb-9edd-4956-8fd4-a1d474ee9199",
   "metadata": {},
   "source": [
    "### Using Custom Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a0f882e-4706-44fe-86a2-653e666aaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':5})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "\n",
    "system_template = r'''\n",
    "Use the following pieces of context to answer the user's question.\n",
    "---------------------------------\n",
    "context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = '''\n",
    "Question: ```{question}```\n",
    "Chat History: ```{chat_history}```\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff',\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b5e32aaf-761d-42f1-8155-f3db735c2da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['chat_history', 'context', 'question'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"\\nUse the following pieces of context to answer the user's question.\\n---------------------------------\\ncontext: ```{context}```\\n\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='\\nQuestion: ```{question}```\\nChat History: ```{chat_history}```\\n'))]\n"
     ]
    }
   ],
   "source": [
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ecc5b953-957d-4a22-8a42-c62e63071f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "---------------------------------\n",
      "context: ```Having two dads offered me the choice of  \n",
      "contrasting points of view:  \n",
      "one of a rich man and one of a poor man.  Introduction\n",
      "RICH DAD POOR DAD1\n",
      "I had two fathers, a rich one and a poor one. One was highly\n",
      "\n",
      "Having two dads offered me the choice of  \n",
      "contrasting points of view:  \n",
      "one of a rich man and one of a poor man.  Introduction\n",
      "RICH DAD POOR DAD1\n",
      "I had two fathers, a rich one and a poor one. One was highly\n",
      "\n",
      "Having two dads offered me the choice of  \n",
      "contrasting points of view:  \n",
      "one of a rich man and one of a poor man.  Introduction\n",
      "RICH DAD POOR DAD1\n",
      "I had two fathers, a rich one and a poor one. One was highly\n",
      "\n",
      "amount of money he earned, which was \n",
      "significant, but because of his thoughts \n",
      "and actions. As a young boy having two \n",
      "fathers, I became acutely aware of being \n",
      "careful about which thoughts I chose to adopt as my own. Should I\n",
      "\n",
      "amount of money he earned, which was \n",
      "significant, but because of his thoughts \n",
      "and actions. As a young boy having two \n",
      "fathers, I became acutely aware of being \n",
      "careful about which thoughts I chose to adopt as my own. Should I```\n",
      "\n",
      "Human: \n",
      "Question: ```how many fathers did author have?```\n",
      "Chat History: ``````\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'how many fathers did author have?', 'chat_history': [HumanMessage(content='how many fathers did author have?'), AIMessage(content='The author had two fathers.')], 'answer': 'The author had two fathers.'}\n"
     ]
    }
   ],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'how many fathers did author have?'\n",
    "result = ask_question_chroma(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "80fe4ddd-4a48-491a-ae7d-604eb0bbc005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: how many fathers did author have?\n",
      "Assistant: The author had two fathers.\n",
      "Follow Up Input: Multiply the number with 2\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "---------------------------------\n",
      "context: ```worked harder to draw more money in, rather than focus on the most \n",
      "important law of money: “Give, and you shall receive.” Instead, he \n",
      "believed in: “Receive, and then you give.”\n",
      "In conclusion, I became both dads. One part of me is a hard-core\n",
      "\n",
      "worked harder to draw more money in, rather than focus on the most \n",
      "important law of money: “Give, and you shall receive.” Instead, he \n",
      "believed in: “Receive, and then you give.”\n",
      "In conclusion, I became both dads. One part of me is a hard-core\n",
      "\n",
      "worked harder to draw more money in, rather than focus on the most \n",
      "important law of money: “Give, and you shall receive.” Instead, he \n",
      "believed in: “Receive, and then you give.”\n",
      "In conclusion, I became both dads. One part of me is a hard-core\n",
      "\n",
      "of study.\n",
      "If I had had only one dad, I would have had to accept or reject his \n",
      "advice. Having two dads offered me the choice of contrasting points \n",
      "of view: one of a rich man and one of a poor man.\n",
      "\n",
      "of study.\n",
      "If I had had only one dad, I would have had to accept or reject his \n",
      "advice. Having two dads offered me the choice of contrasting points \n",
      "of view: one of a rich man and one of a poor man.```\n",
      "\n",
      "Human: \n",
      "Question: ```What is the result of multiplying the number of fathers the author had by 2?```\n",
      "Chat History: ```\n",
      "Human: how many fathers did author have?\n",
      "Assistant: The author had two fathers.```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'Multiply the number with 2', 'chat_history': [HumanMessage(content='how many fathers did author have?'), AIMessage(content='The author had two fathers.'), HumanMessage(content='Multiply the number with 2'), AIMessage(content='The result of multiplying the number of fathers the author had by 2 is 4.')], 'answer': 'The result of multiplying the number of fathers the author had by 2 is 4.'}\n"
     ]
    }
   ],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'Multiply the number with 2'\n",
    "result = ask_question_chroma(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a335c30-4fb2-4275-9597-3a3f6aad6fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: how many fathers did author have?\n",
      "Assistant: The author had two fathers.\n",
      "Human: Multiply the number with 2\n",
      "Assistant: The result of multiplying the number of fathers the author had by 2 is 4.\n",
      "Human: Can you give the answer in Odia?\n",
      "Assistant: The author had two fathers. Multiplying this number by 2 gives us a total of 4 fathers.\n",
      "Follow Up Input: Can you give the previous answer in Odia?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "---------------------------------\n",
      "context: ```a book and a game about accounting, another subject I struggled with.\n",
      "So what does this have to do with the question: “What is your goal \n",
      "in life?”Preface\n",
      "WHAT IS YOUR\n",
      "LIFE'S GOAL?\n",
      "xv\n",
      "\n",
      "a book and a game about accounting, another subject I struggled with.\n",
      "So what does this have to do with the question: “What is your goal \n",
      "in life?”Preface\n",
      "WHAT IS YOUR\n",
      "LIFE'S GOAL?\n",
      "xv\n",
      "\n",
      "a book and a game about accounting, another subject I struggled with.\n",
      "So what does this have to do with the question: “What is your goal \n",
      "in life?”Preface\n",
      "WHAT IS YOUR\n",
      "LIFE'S GOAL?\n",
      "xv\n",
      "\n",
      "a liability? The answer is found in basic education.\n",
      "We focus on the word “literacy” and not “financial literacy.”  \n",
      "What defines something to be an asset or a liability are not words.\n",
      "\n",
      "a liability? The answer is found in basic education.\n",
      "We focus on the word “literacy” and not “financial literacy.”  \n",
      "What defines something to be an asset or a liability are not words.```\n",
      "\n",
      "Human: \n",
      "Question: ```Can you provide the answer in Odia?```\n",
      "Chat History: ```\n",
      "Human: how many fathers did author have?\n",
      "Assistant: The author had two fathers.\n",
      "Human: Multiply the number with 2\n",
      "Assistant: The result of multiplying the number of fathers the author had by 2 is 4.\n",
      "Human: Can you give the answer in Odia?\n",
      "Assistant: The author had two fathers. Multiplying this number by 2 gives us a total of 4 fathers.```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I'm sorry, but I cannot provide the answer in Odia as the context provided does not contain any text in Odia. If you have any other questions or need assistance with something else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'Can you give the previous answer in Odia?'\n",
    "result = ask_question_chroma(q, crc)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b8e0c9c9-a069-431d-9b1e-16553e096252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Can you give the answer in Odia?',\n",
       " 'chat_history': [HumanMessage(content='how many fathers did author have?'),\n",
       "  AIMessage(content='The author had two fathers.'),\n",
       "  HumanMessage(content='Multiply the number with 2'),\n",
       "  AIMessage(content='The result of multiplying the number of fathers the author had by 2 is 4.'),\n",
       "  HumanMessage(content='Can you give the answer in Odia?'),\n",
       "  AIMessage(content='The author had two fathers. Multiplying this number by 2 gives us a total of 4 fathers.')],\n",
       " 'answer': 'The author had two fathers. Multiplying this number by 2 gives us a total of 4 fathers.'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
